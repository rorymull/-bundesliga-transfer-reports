name: bundesliga-defender-rumours

on:
  schedule:
    - cron: '30 7 * * *'   # 07:30 UTC daily
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      # If you added requirements.txt, keep caching; otherwise remove cache lines.
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          # If you DON'T have requirements.txt, replace the next line with:
          # pip install requests beautifulsoup4 lxml
          pip install -r requirements.txt

      - name: Scrape Transfermarkt defenders
        env:
          COMPETITION: L1
          SEASON_ID: '2025'
        run: |
          # Adjust the path to where your script actually lives
          if [ -f scripts/scrape_defenders.py ]; then
            python scripts/scrape_defenders.py
          else
            python scrape_defenders.py
          fi

      - name: Verify output exists
        run: |
          ls -lah
          ls -lah out || true
          test -f out/defender_rumours.json

      # ⬅️ Upload the Pages artifact from THIS job
      - name: Upload artifact for GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: out  # this contains defender_rumours.json

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
